# -*- coding: utf-8 -*-
"""URL-Level Comparison between Traditional Search and LLM Web Search APIs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N85DFHC9Pi1HVDPKWgJxtK8IFzqQpnyv

## Google Search vs. Sonar Pro vs. Bing Search vs. OpenAI Web Search

### Helper functions
"""

import csv
from datetime import datetime
from serpapi import GoogleSearch
import pytz

local_tz = pytz.timezone("America/Toronto")

def get_serpapi_urls(query, engine, num_results=10):
    params = {
        "engine": engine,
        "q": query,
        # "gl": "ca",
        # "cc": "CA",
        "api_key": SERPAPI_API_KEY,
        "num": num_results
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    organic_results = results.get("organic_results", [])
    urls = []
    for res in organic_results:
        url = res.get("link", None)
        if url:
            urls.append(url)
    return urls

def save_multiple_queries_to_csv(queries_by_category, engine="google", num_results=10, output_filename="serpapi_sources.csv"):
    timestamp = datetime.now(local_tz).strftime("%Y-%m-%d %H:%M:%S")

    with open(output_filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Query", "Category", "Source", "Source Rank", "Timestamp", "Search Source"])

        for category, query in queries_by_category.items():
            urls = get_serpapi_urls(query, engine=engine, num_results=num_results)
            for rank, url in enumerate(urls, start=1):
                writer.writerow([query, category, url, rank, timestamp, engine.title() + " Search"])

    print(f"âœ… All results saved to {output_filename}")

# get_serpapi_urls('best instant coffee', engine='bing')

from urllib.parse import urlparse, urlunparse
import pandas as pd
import ast
from matplotlib_venn import venn2
import matplotlib.pyplot as plt

def normalize_url(url):
    try:
        parsed = urlparse(url)
        clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path.rstrip("/"), '', '', ''))
        return clean_url.lower()
    except:
        return str(url).lower().strip()

def compare_2_urls_by_category(
    df1, df2,
    url_col1, url_col2,
    plot=False,
    venn_labels=('File 1', 'File 2'),
    summary_labels=('URLs in File1', 'URLs in File2')
):
    import ast
    import pandas as pd
    from matplotlib_venn import venn2
    import matplotlib.pyplot as plt

    def normalize_url(url):
        return url.strip().rstrip("/").lower()

    def extract_urls(cell):
        try:
            urls = ast.literal_eval(cell) if isinstance(cell, str) and cell.startswith("[") else [cell]
            return [normalize_url(u) for u in urls if isinstance(u, str)]
        except Exception:
            return []

    # Normalize category column
    df1["Category"] = df1["Category"].str.lower()
    df2["Category"] = df2["Category"].str.lower()

    # Explode any lists of URLs
    df1["Normalized URL"] = df1[url_col1].apply(extract_urls)
    df1 = df1.explode("Normalized URL")

    df2["Normalized URL"] = df2[url_col2].apply(extract_urls)
    df2 = df2.explode("Normalized URL")

    all_categories = sorted(set(df1["Category"].unique()) | set(df2["Category"].unique()))
    summary = []

    for category in all_categories:
        urls1 = df1[df1["Category"] == category]
        urls2 = df2[df2["Category"] == category]

        set1 = set(urls1["Normalized URL"].dropna().unique())
        set2 = set(urls2["Normalized URL"].dropna().unique())

        common = set1 & set2
        only1 = set1 - set2
        only2 = set2 - set1

        total_unique = len(set1 | set2)
        overlap_rate = len(common) / total_unique if total_unique else 0

        summary.append({
            "Category": category,
            summary_labels[0]: len(set1),
            summary_labels[1]: len(set2),
            "Common URLs": len(common),
            "Common URL List": sorted(common),
            "Overlap Rate %": round(overlap_rate * 100, 2)
        })

        # Print results
        print(f"\nðŸ“¦ Category: {category}")
        print(f"âœ… Common URLs ({len(common)}):")
        for u in sorted(common):
            print("   ", u)

        print(f"\nðŸ”´ Only in {venn_labels[0]} ({len(only1)}):")
        print(
            urls1[urls1["Normalized URL"].isin(only1)]
            [[url_col1]]
            .drop_duplicates(subset=url_col1)
        )

        print(f"\nðŸ”µ Only in {venn_labels[1]} ({len(only2)}):")
        print(
            urls2[urls2["Normalized URL"].isin(only2)]
            [[url_col2]]
            .drop_duplicates(subset=url_col2)
        )

        if plot:
            venn2([set1, set2], set_labels=venn_labels)
            plt.title(f"URL Overlap for {category}")
            plt.show()

    return pd.DataFrame(summary)

"""### Data Preparation"""

queries = {
    "Instant Coffee": "Top recommended Instant Coffee in Canada"
    # "Cat Food": "best cat food",
    # "Dog Food": "best dog food",
    # "Baking Ingredients": "best baking ingredients",
    # "Cat Litter": "best cat litter"
}

save_multiple_queries_to_csv(queries_by_category=queries, engine="google", num_results=20, output_filename="serpapi_google_5_5.csv")
save_multiple_queries_to_csv(queries_by_category=queries, engine="bing", num_results=20, output_filename="serpapi_bing_5_5.csv")
serpapi_google_results = pd.read_csv("serpapi_google_5_5.csv")
serpapi_bing_results = pd.read_csv("serpapi_bing_5_5.csv")

# serpapi_google_results = pd.read_csv("GoogleSearch_instant_coffee.csv")
# serpapi_bing_results = pd.read_csv("BingSearch_instant_coffee.csv")

serpapi_google_results['Category'] = 'Instant Coffee'
serpapi_bing_results['Category'] = 'Instant Coffee'

# SonarPro_data = pd.read_csv("Sonar_Pro_4_16.csv")
# OpenAI_data = pd.read_csv("OpenAI_Web_Search_4_16.csv")

# SonarPro_data = SonarPro_data.rename(columns={"category": "Category"})
# OpenAI_data = OpenAI_data.rename(columns={"category": "Category"})

OpenAI_data_raw = pd.read_csv("OpenAI_web_search_4_21_processed.csv")
OpenAI_data_raw.head()

# import pandas as pd

# def compute_llm_ranking_summary(input_csv_path, output_csv_path, round_keyword="round"):

#     df = pd.read_csv(input_csv_path)

#     # Obtain ranking columns
#     rank_cols = [col for col in df.columns if round_keyword.lower() in col.lower()]

#     df["Mention Count"] = df[rank_cols].notna().sum(axis=1)
#     df["Avg Rank (Skip NaN)"] = df[rank_cols].mean(axis=1).round(2)
#     df["Avg Rank (Fill 0)"] = df[rank_cols].fillna(0).mean(axis=1).round(2)
#     df["Avg Rank (Fill 11)"] = df[rank_cols].fillna(11).mean(axis=1).round(2)

#     # URLs count
#     if "urls" in df.columns:
#         def count_urls(cell):
#             try:
#                 urls = ast.literal_eval(cell)
#                 return len(urls) if isinstance(urls, list) else 0
#             except:
#                 return 0

#         df["URLs count"] = df["urls"].apply(count_urls)

#     df.to_csv(output_csv_path, index=False)
#     print(f"âœ… Summary saved to: {output_csv_path}")

#     return df

# df_summary = compute_llm_ranking_summary(
#     input_csv_path="OpenAI_web_search_4_21.csv",
#     output_csv_path="OpenAI_web_search_4_21_processed.csv"
# )

# OpenAI_data = df_summary[['brand', 'short_title', 'urls', 'category', 'URLs count', 'Mention Count', 'Avg Rank (Skip NaN)', 'Avg Rank (Fill 0)', 'Avg Rank (Fill 11)']]
# OpenAI_data = OpenAI_data.rename(columns={"category": "Category"})
# OpenAI_data.head()

OpenAI_data = OpenAI_data_raw[['brand', 'short_title', 'urls', 'category', 'URLs count', 'Mention Count', 'Avg Rank (Skip NaN)', 'Avg Rank (Fill 0)', 'Avg Rank (Fill 11)']]
OpenAI_data = OpenAI_data.rename(columns={"category": "Category"})
OpenAI_data.head()

"""### Comparison"""

compare_2_urls_by_category(
    serpapi_google_results,
    serpapi_bing_results,
    url_col1="Source",
    url_col2="Source",
    plot=True,
    venn_labels=('Google Search', 'Bing Search'),
    summary_labels=('URLs in Google Search', 'URLs in Bing Search')
)

compare_2_urls_by_category(
    OpenAI_data,
    serpapi_google_results,
    url_col1="urls",
    url_col2="Source",
    plot=True,
    venn_labels=('OpenAI Web Search', 'Google Search'),
    summary_labels=('URLs in OpenAI Web Search', 'URLs in Google Search')
)

compare_2_urls_by_category(
    OpenAI_data,
    serpapi_bing_results,
    url_col1="urls",
    url_col2="Source",
    plot=True,
    venn_labels=('OpenAI Web Search', 'Bing Search'),
    summary_labels=('URLs in OpenAI Web Search', 'URLs in Bing Search')
)

# bing_data = pd.read_csv("serpapi_bing_results.csv")

# compare_2_urls_by_category(
#     SonarPro_data,
#     serpapi_bing_results,
#     url_col1="urls",
#     url_col2="Source",
#     plot=True,
#     venn_labels=('SonarPro', 'Bing Search'),
#     summary_labels=('URLs in SonarPro', 'URLs in Bing')
# )

# compare_2_urls_by_category(
#     SonarPro_data,
#     serpapi_google_results,
#     url_col1="urls",
#     url_col2="Source",
#     plot=True,
#     venn_labels=('SonarPro', 'Google Search'),
#     summary_labels=('URLs in SonarPro', 'URLs in Google Search')
# )

# compare_2_urls_by_category(
#     SonarPro_data,
#     OpenAI_data,
#     url_col1="urls",
#     url_col2="urls",
#     plot=True,
#     venn_labels=('SonarPro', 'OpenAI Web Search'),
#     summary_labels=('URLs in SonarPro', 'URLs in OpenAI Web Search')
# )

"""### Precise comparison - OpenAI versus Google"""

import pandas as pd
import ast
from datetime import datetime

def get_common_url_rows_with_overlap_type(
    openai_df: pd.DataFrame,
    serpapi_df: pd.DataFrame,
    openai_url_col: str = "urls",
    serpapi_url_col: str = "Source"
) -> pd.DataFrame:
    """
    Return a DataFrame containing only rows from openai_df and serpapi_df whose URLs overlap,
    plus a 'URL Overlap Type' column and the combined set of requested fields.
    """

    def normalize_url(url: str) -> str:
        return url.strip().rstrip("/").lower()

    def extract_urls(cell) -> list[str]:
        try:
            if isinstance(cell, str) and cell.startswith("["):
                lst = ast.literal_eval(cell)
            else:
                lst = [cell]
            return [normalize_url(u) for u in lst if isinstance(u, str)]
        except:
            return []

    # â€” explode OpenAI data â€”
    o = openai_df.copy()
    o["__norm_url"] = o[openai_url_col].apply(extract_urls)
    o = o.explode("__norm_url").rename(columns={openai_url_col: "orig_urls"})

    # â€” explode SERPAPI data â€”
    s = serpapi_df.copy()
    s["__norm_url"] = s[serpapi_url_col].apply(lambda u: [normalize_url(u)] if isinstance(u, str) else [])
    s = s.explode("__norm_url").rename(columns={serpapi_url_col: "orig_source"})

    # â€” inner join on normalized URL and categoryâ€”
    # Assuming 'Category' column is in both DataFrames
    merged = pd.merge(
        o, s,
        on=["__norm_url", "Category"],  # Include 'Category' in the join condition
        how="inner",
        suffixes=("_openai", "_serpapi")
    )
    # Check if merged DataFrame is empty after the join
    if merged.empty:
        print("Warning: No common URLs and categories found between the two DataFrames.")
        return pd.DataFrame()  # Return an empty DataFrame if no common elements found

    # â€” classify overlap type â€”
    def classify(row) -> str:
        openai_set = set(extract_urls(row["orig_urls"]))
        serpapi_set = {normalize_url(row["orig_source"])}
        inter = openai_set & serpapi_set
        if not inter:
            return "No Match"
        if openai_set == serpapi_set:
            return "Exact Match"
        return "Partial Match"

    merged["URL Overlap Type"] = merged.apply(classify, axis=1)

    # â€” select & rename the requested columns â€”
    # Ensure all columns are present in the merged DataFrame before selection
    cols_to_select = [
        "brand", "short_title", "orig_urls", "Category", "URLs count", "Mention Count",
        "Avg Rank (Skip NaN)", "Avg Rank (Fill 0)", "Avg Rank (Fill 11)", "Query",
        "orig_source", "Source Rank", "Timestamp", "Search Source", "URL Overlap Type"
    ]
    # Only select columns that are present in the merged DataFrame
    available_cols = [col for col in cols_to_select if col in merged.columns]
    df_out = merged[available_cols].copy()

    df_out.rename(columns={
        "orig_urls": "urls",
        "orig_source": "Source link in Traditional Search",
        "Source Rank": "Source Rank in Traditional Search",
        "Timestamp": "Timestamp in Traditional Search"
    }, inplace=True)

    return df_out.reset_index(drop=True)

OpenAI_Google_combined = get_common_url_rows_with_overlap_type(
    openai_df=OpenAI_data,
    serpapi_df=serpapi_google_results,
    openai_url_col="urls",
    serpapi_url_col="Source"
)
OpenAI_Google_combined.head()

OpenAI_Google_combined[(OpenAI_Google_combined['URL Overlap Type'] == 'Exact Match') | (OpenAI_Google_combined['URL Overlap Type'] == 'Partial Match')]

# Manually add ranking of the product in the URL
OpenAI_Google_match = OpenAI_Google_combined[(OpenAI_Google_combined['URL Overlap Type'] == 'Exact Match') | (OpenAI_Google_combined['URL Overlap Type'] == 'Partial Match')]
OpenAI_Google_match_clean = OpenAI_Google_match.drop(columns=['Category','Avg Rank (Skip NaN)','Query','urls','Timestamp in Traditional Search','Search Source'])
OpenAI_Google_match_clean['Rank in common URL'] = [1,2,3,4,5,6,7,8,'Not Found','Not Found',10,9]
OpenAI_Google_match_clean.to_csv('OpenAI_Google_match_clean.csv', index=False)
OpenAI_Google_match_clean

"""#### OpenAI versus Bing"""

OpenAI_Bing_combined = get_common_url_rows_with_overlap_type(
    openai_df=OpenAI_data,
    serpapi_df=serpapi_bing_results,
    openai_url_col="urls",
    serpapi_url_col="Source"
)
OpenAI_Bing_combined.head()

# Manually add ranking of the product in the URL
OpenAI_Bing_match = OpenAI_Bing_combined[(OpenAI_Bing_combined['URL Overlap Type'] == 'Exact Match') | (OpenAI_Bing_combined['URL Overlap Type'] == 'Partial Match')]
OpenAI_Bing_match_clean = OpenAI_Bing_match.drop(columns=['Category','Avg Rank (Skip NaN)','Query','urls','Timestamp in Traditional Search','Search Source'])
OpenAI_Bing_match_clean.to_csv('OpenAI_Bing_match_clean.csv', index=False)
OpenAI_Bing_match_clean