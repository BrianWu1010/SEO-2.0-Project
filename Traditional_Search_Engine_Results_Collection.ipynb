{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Packages prep"
      ],
      "metadata": {
        "id": "Fyi4Tw0zWm8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peoJ4duNa3YK",
        "outputId": "64773b10-2c71-4bff-ba92-4a8150509418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.4.26)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.78.1\n",
            "    Uninstalling openai-1.78.1:\n",
            "      Successfully uninstalled openai-1.78.1\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-search-results pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R3vMx_YLAzJ",
        "outputId": "ed40ca00-cdaf-4e81-ddb7-8d78a0c3680c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.4.26)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=64741c20c4dc6b0321b478236984443c81373224a17b0b55c566e17302b77c07\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkQN635zl5DM",
        "outputId": "444c10f5-d90b-4cb4-a24e-e3689c786302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3us5D9MQdes",
        "outputId": "ee6a6135-c13c-4888-abda-4afe860e0973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openperplex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxXU_PcxgFe9",
        "outputId": "6bde0de5-cbea-41c9-b179-766e655f97dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openperplex\n",
            "  Downloading openperplex-0.3.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: httpx>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from openperplex) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->openperplex) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->openperplex) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->openperplex) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->openperplex) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.26.0->openperplex) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.26.0->openperplex) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.26.0->openperplex) (4.13.2)\n",
            "Downloading openperplex-0.3.0-py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: openperplex\n",
            "Successfully installed openperplex-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import openai\n",
        "import praw\n",
        "from serpapi import GoogleSearch\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "local_tz = pytz.timezone(\"America/Toronto\")\n",
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib_venn import venn2, venn3\n",
        "from openperplex import OpenperplexSync\n",
        "\n",
        "# Set your API keys\n",
        "openai.api_key = \"your openai api key\"\n",
        "SERPAPI_API_KEY = \"your serpapi key\""
      ],
      "metadata": {
        "id": "cLM8NbzH5vge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Searching Data Prep"
      ],
      "metadata": {
        "id": "8YNKDEmv2Kv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "VCQhI_5q-bXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define headers for requests\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "def get_serpapi_urls(query, engine='google', num_results=10):\n",
        "    \"\"\"Return a list of website URLs from SERPAPI using the given query.\"\"\"\n",
        "    params = {\n",
        "        \"engine\": engine,\n",
        "        \"q\": query,\n",
        "        \"cc\": \"CA\",\n",
        "        \"api_key\": SERPAPI_API_KEY,\n",
        "        \"num\": num_results\n",
        "    }\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "    organic_results = results.get(\"organic_results\", [])\n",
        "    urls = []\n",
        "    for res in organic_results:\n",
        "        url = res.get(\"link\", None)\n",
        "        if url:\n",
        "            urls.append(url)\n",
        "    return urls"
      ],
      "metadata": {
        "id": "H8sIP0ce6RN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cleaned_html(url):\n",
        "    \"\"\"Attempt to scrape and clean the HTML from a given URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        # Remove script and style tags\n",
        "        for tag in soup([\"style\"]):\n",
        "            tag.decompose()\n",
        "        # Try to extract main content: first look for <article>\n",
        "        article = soup.find(\"article\")\n",
        "        if not article:\n",
        "            # Fallback: try a div with class \"content\"\n",
        "            article = soup.find(\"div\", {\"class\": \"content\"})\n",
        "        if not article:\n",
        "            # As last resort, use the whole page\n",
        "            article = soup\n",
        "        cleaned = article.get_text(separator=\"\\n\", strip=True)\n",
        "        return cleaned\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "NbKxh9bvpg_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_product_data(cleaned_html):\n",
        "    \"\"\"\n",
        "    Use GPT-4o to extract product ranking data from the cleaned HTML.\n",
        "    This function calls the model with function_call parameters and returns a dictionary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are an expert at extracting product rankings and critical reviews from article HTML content. \"\n",
        "                        # \"Focus on extracting subjective evaluations, opinions, and direct comparisons between products rather than just product descriptions. \"\n",
        "                        # \"Critical reviews should include strengths, weaknesses, pros, and cons mentioned about each product. \"\n",
        "                        # \"Ignore neutral descriptions that only state product features without an evaluation. \"\n",
        "                        \"If explicit rankings exist, maintain them. If no ranking is mentioned, rank the products in the order they appear in the article. Never leave it blank.\"\n",
        "                        \"If there are more than 10 products mentioned, only keep the top 10.\"\n",
        "                    )\n",
        "\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": cleaned_html}\n",
        "            ],\n",
        "            functions=[\n",
        "                {\n",
        "                    \"name\": \"parse_product_data\",\n",
        "                    \"description\": (\n",
        "                        \"Extracts ranked products and their corresponding critical reviews from article HTML content. \"\n",
        "                        \"If explicit rankings exist, keep them. If no ranking is given, rank them by the order of appearance. Never leave it blank.\"\n",
        "                        \"If a product has no review in the article, mark it as 'No review available' instead of leaving it empty.\"\n",
        "                    ),\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"products\": {\n",
        "                                \"type\": \"array\",\n",
        "                                \"items\": {\n",
        "                                    \"type\": \"object\",\n",
        "                                    \"properties\": {\n",
        "                                        \"Result Rank\": {\"type\": \"integer\"},\n",
        "                                        \"Product Title\": {\"type\": \"string\"},\n",
        "                                        \"Comment\": {\"type\": \"string\"}\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ],\n",
        "            function_call={\"name\": \"parse_product_data\"}\n",
        "        )\n",
        "        func_call = completion.choices[0].message.get(\"function_call\", {})\n",
        "        arguments_str = func_call.get(\"arguments\", \"{}\")\n",
        "        data = json.loads(arguments_str)\n",
        "        return data.get(\"products\", [])\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting product data: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "5qReV6l6uXzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reddit_comments(url):\n",
        "    \"\"\"\n",
        "    Fetch the top 10 comments from a given Reddit URL.\n",
        "    \"\"\"\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=\"ZUwpxO018ZJGE4E-dfwazQ\",\n",
        "        client_secret=\"SUE9d8Q_wr_4N1qcebomwtS293GwJw\",\n",
        "        user_agent=\"MyRedditScraper/0.1\",\n",
        "        check_for_async=False\n",
        "    )\n",
        "\n",
        "    submission = reddit.submission(url=url)\n",
        "    submission.comments.replace_more(limit=0)\n",
        "    comments = submission.comments.list()\n",
        "    # Sort comments by score descending, take top 10.\n",
        "    top_comments = sorted(comments, key=lambda x: x.score, reverse=True)[:10]\n",
        "    # Concatenate comment texts with their score.\n",
        "    comments_text = \"\\n\\n\".join([f\"Score: {c.score}\\nComment: {c.body}\" for c in top_comments])\n",
        "    return comments_text"
      ],
      "metadata": {
        "id": "ghdzyHdAQA8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_product_data_from_reddit(url):\n",
        "    \"\"\"\n",
        "    Use GPT-4o to extract product ranking data from the Reddit comments.\n",
        "    This function calls the model with function_call parameters and returns a list of product dictionaries.\n",
        "    Only products with positive (or at least non-negative) reviews are returned;\n",
        "    if a product is judged to have a negative review, it is skipped.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        reddit_comments = get_reddit_comments(url)\n",
        "        # Call OpenAI to analyze the Reddit comments.\n",
        "        # Note: The system prompt instructs the model to only return products that are positively reviewed.\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are an expert at extracting product names, rankings and critical reviews information from aggregated Reddit comments. \"\n",
        "                        \"Analyze the following top 10 comments and extract the product ranking, product name, a summary of the critical reviews (Comment). \"\n",
        "                        \"Only include products that are positively reviewed. If a product is judged to have a negative review, do not include it in the results. \"\n",
        "                        \"Rank products in the order they appear.\"\n",
        "                        \"If a product has no clear review, mark it as 'No review available'.\"\n",
        "                    )\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": reddit_comments}\n",
        "            ],\n",
        "            functions=[\n",
        "                {\n",
        "                    \"name\": \"parse_product_data\",\n",
        "                    \"description\": (\n",
        "                        \"Extracts ranked products, and their corresponding critical reviews from Reddit comments. \"\n",
        "                        \"Only return products with positive reviews. Otherwise, maintain the order of appearance.\"\n",
        "                    ),\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"products\": {\n",
        "                                \"type\": \"array\",\n",
        "                                \"items\": {\n",
        "                                    \"type\": \"object\",\n",
        "                                    \"properties\": {\n",
        "                                        \"Result Rank\": {\"type\": \"integer\"},\n",
        "                                        \"Product Title\": {\"type\": \"string\"},\n",
        "                                        \"Comment\": {\"type\": \"string\"}\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ],\n",
        "            function_call={\"name\": \"parse_product_data\"}\n",
        "        )\n",
        "        func_call = completion.choices[0].message.get(\"function_call\", {})\n",
        "        arguments_str = func_call.get(\"arguments\", \"{}\")\n",
        "        data = json.loads(arguments_str)\n",
        "        return data.get(\"products\", [])\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting product data from reddit URL {url}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "b9TaN_BHmzHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_serpapi_products(query, engine='google', required_count=5, num_results=10, csv_filename=\"output.csv\"):\n",
        "    \"\"\"\n",
        "    Uses SERPAPI to fetch website URLs based on the given query and engine, scrapes them to extract product data,\n",
        "    and writes the results to a CSV file. Each product record is augmented with:\n",
        "        - 'Source': the URL from which the data was scraped.\n",
        "        - 'Source Rank': the ordinal number (1-based) of the SERPAPI URL that produced this data.\n",
        "        - 'Search Source': the engine name (e.g. \"google\", \"google_shopping\", etc.)\n",
        "        - 'Query': the SERPAPI query string used.\n",
        "        - 'Timestamp': the timestamp when the query was sent to SERPAPI.\n",
        "\n",
        "    The final CSV file will contain the following columns:\n",
        "      - Result Rank\n",
        "      - Title        (was \"Product Title\")\n",
        "      - Description  (was \"Comment\")\n",
        "      - Source       (was \"Source Link\")\n",
        "      - Source Rank\n",
        "      - Search Source\n",
        "      - Query\n",
        "      - Timestamp\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The SERPAPI query string.\n",
        "        engine (str): The SERPAPI engine to use (e.g. \"google\", \"google_shopping\").\n",
        "        required_count (int): The number of successfully scraped website results to collect.\n",
        "        num_results (int): The number of URLs to ask SERPAPI for.\n",
        "        csv_filename (str): The filename for the CSV output.\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    import time, csv\n",
        "\n",
        "    query_used = query\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Fetch URLs using the specified engine\n",
        "    urls = get_serpapi_urls(query, engine, num_results=num_results)\n",
        "    print(f\"SERPAPI returned {len(urls)} URLs for engine='{engine}'.\")\n",
        "\n",
        "    results_set = set()\n",
        "\n",
        "    for i, url in enumerate(urls, start=1):\n",
        "        # handle Reddit specially\n",
        "        if \"reddit\" in url.lower():\n",
        "            print(f\"Processing Reddit URL {i}: {url}\")\n",
        "            products_data = extract_product_data_from_reddit(url)\n",
        "        else:\n",
        "            if len(results_set) >= required_count:\n",
        "                break\n",
        "            print(f\"Processing URL {i}: {url}\")\n",
        "            cleaned_html = get_cleaned_html(url)\n",
        "            if not cleaned_html or len(cleaned_html) < 100:\n",
        "                print(f\"Unable to extract content from {url}, skipping.\")\n",
        "                continue\n",
        "            products_data = extract_product_data(cleaned_html)\n",
        "\n",
        "        if not products_data:\n",
        "            print(f\"Extraction failed for {url}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        for product in products_data:\n",
        "            product[\"Source Link\"] = url\n",
        "            product[\"Source Rank\"] = i\n",
        "            product[\"Timestamp\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        # ensure uniqueness\n",
        "        hashable = tuple(frozenset(prod.items()) for prod in products_data)\n",
        "        results_set.add(hashable)\n",
        "        time.sleep(1)\n",
        "\n",
        "    if len(results_set) < required_count:\n",
        "        print(\"Warning: fewer than required valid results were scraped.\")\n",
        "\n",
        "    # flatten\n",
        "    all_products = []\n",
        "    for group in results_set:\n",
        "        for item in group:\n",
        "            all_products.append(dict(item))\n",
        "\n",
        "    # write CSV\n",
        "    with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"Result Rank\",\n",
        "            \"Title\",\n",
        "            \"Description\",\n",
        "            \"Source\",\n",
        "            \"Source Rank\",\n",
        "            \"Search Source\",\n",
        "            \"Query\",\n",
        "            \"Timestamp\"\n",
        "        ])\n",
        "        for prod in all_products:\n",
        "            writer.writerow([\n",
        "                prod.get(\"Result Rank\", \"N/A\"),\n",
        "                prod.get(\"Product Title\", \"N/A\"),   # renamed to Title\n",
        "                prod.get(\"Comment\", \"N/A\"),         # renamed to Description\n",
        "                prod.get(\"Source Link\", \"N/A\"),     # renamed to Source\n",
        "                prod.get(\"Source Rank\", \"N/A\"),\n",
        "                engine,                             # Search Source comes from the engine parameter\n",
        "                query_used,\n",
        "                prod.get(\"Timestamp\", timestamp)\n",
        "            ])\n",
        "\n",
        "    print(f\"CSV file saved: {csv_filename}\")\n"
      ],
      "metadata": {
        "id": "lmpCktVsYHfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Five categories - 10 URLs"
      ],
      "metadata": {
        "id": "O1-9s3EC-EKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"Best baking ingredients\"\n",
        "# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename=\"GoogleSearch_baking_ingredients.csv\")"
      ],
      "metadata": {
        "id": "3bMVUAYgptX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Top recommended instant coffee in Canada\"\n",
        "scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename=\"GoogleSearch_instant_coffee.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e6d2a5-6eef-48d7-c4e7-5c833d159f9c",
        "id": "cedxQ07oNEtT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 18 URLs for engine='google'.\n",
            "Processing Reddit URL 1: https://www.reddit.com/r/BuyCanadian/comments/1ilpd72/best_grocery_store_instant_coffee_brand_made_in/\n",
            "Processing URL 2: https://www.narcity.com/instant-coffee-comparison-canada\n",
            "Processing URL 3: https://www.andreaference.com/blog/the-best-instant-coffee\n",
            "Processing URL 4: https://www.bonappetit.com/story/best-instant-coffee?srsltid=AfmBOooEyIlpse_BETT1nYIYBV8jwfA1ZuCvSxAnTOSgatpHWxXjDzSl\n",
            "Processing URL 5: https://cornercoffeestore.com/best-instant-coffees-canada/\n",
            "Unable to extract content from https://cornercoffeestore.com/best-instant-coffees-canada/, skipping.\n",
            "Processing URL 6: https://www.amazon.ca/best-instant-coffee/s?k=best+instant+coffee\n",
            "Processing URL 7: https://www.allrecipes.com/longform/best-instant-coffee/\n",
            "Processing URL 8: https://justuscoffee.com/collections/favourites/products/organic-instant-coffee-canada\n",
            "Processing URL 9: https://www.thekitchn.com/best-instant-coffee-23632498\n",
            "Processing URL 10: https://www.businessinsider.com/what-is-the-best-instant-coffee-review-2022-2\n",
            "Unable to extract content from https://www.businessinsider.com/what-is-the-best-instant-coffee-review-2022-2, skipping.\n",
            "Processing URL 11: https://www.thespruceeats.com/best-instant-coffee-4690937\n",
            "Processing URL 12: https://www.seriouseats.com/best-instant-coffee-7506045\n",
            "CSV file saved: GoogleSearch_instant_coffee.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"Best dog food\"\n",
        "# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename=\"GoogleSearch_dog_food.csv\")"
      ],
      "metadata": {
        "id": "OyFxgtRHNFRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"Best cat food\"\n",
        "# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename=\"GoogleSearch_cat_food.csv\")"
      ],
      "metadata": {
        "id": "UaLXRvBPNFY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"Best cat litter\"\n",
        "# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename=\"GoogleSearch_cat_litter.csv\")"
      ],
      "metadata": {
        "id": "xRARaAYmNFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Five categories - 5 URLs"
      ],
      "metadata": {
        "id": "TqtJycslVlO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Best baking ingredients\"\n",
        "scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename=\"GoogleSearch_baking_ingredients.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebf8656-92f7-47c3-a984-1a03a09b04af",
        "id": "g8ZHVfMQVlO9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 8 URLs.\n",
            "Processing Reddit URL 1: https://www.reddit.com/r/Baking/comments/10mjc7b/what_splurge_baking_ingredients_would_you_love_as/\n",
            "Processing URL 2: https://www.tasteofhome.com/collection/gourmet-baking-ingredients/?srsltid=AfmBOopTEbMm97qxJ87t95_iwCxCVzM-5UY-aPRMV13kewDLzyoA0Jc9\n",
            "Processing URL 3: https://shop.kingarthurbaking.com/ingredients?srsltid=AfmBOorhZWoyvkzjNiUGtTcIukPkar1lBRVU_QjIZvUzPiIZ3O7i_Gdh\n",
            "Unable to extract content from https://shop.kingarthurbaking.com/ingredients?srsltid=AfmBOorhZWoyvkzjNiUGtTcIukPkar1lBRVU_QjIZvUzPiIZ3O7i_Gdh, skipping.\n",
            "Processing URL 4: https://www.foodnetwork.com/recipes/packages/baking-guide/baking-ingredient-guide\n",
            "Processing URL 5: https://theepicureantrader.com/collections/baking-ingredients?srsltid=AfmBOoq1FgjgAbshTEPFUbsYAMn3at2xYn-WG1meU2P5SsdHfg4_wO0Q\n",
            "Processing URL 6: https://www.allrecipes.com/article/essential-baking-ingredients/\n",
            "CSV file saved: GoogleSearch_baking_ingredients.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Best instant coffee\"\n",
        "scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename=\"GoogleSearch_instant_coffee.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87c14cc-bbd3-42bd-d579-9862d5988feb",
        "id": "-SuQdeiMVlPB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 9 URLs.\n",
            "Processing URL 1: https://www.bonappetit.com/story/best-instant-coffee?srsltid=AfmBOoofsgNvzRb7b1rw71l-UtiXOXoV4ullwoSas0Jt_5L__pcnk3ey\n",
            "Processing URL 2: https://www.tastingtable.com/1755382/instant-coffee-brands-ranked-worst-best/\n",
            "Processing Reddit URL 3: https://www.reddit.com/r/Coffee/comments/y9cjhe/are_there_any_good_instant_coffees/\n",
            "Processing URL 4: https://www.nytimes.com/wirecutter/reviews/best-instant-coffee/\n",
            "Processing URL 5: https://www.outdoorlife.com/gear/best-instant-coffee-for-backpacking/\n",
            "CSV file saved: GoogleSearch_instant_coffee.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Best dog food\"\n",
        "scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename=\"GoogleSearch_dog_food.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8177de48-3d31-422c-ea01-12ed9b3f9736",
        "id": "P2oxeQZuVlPB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 7 URLs.\n",
            "Processing URL 1: https://www.dogfoodadvisor.com/\n",
            "Processing URL 2: https://www.petmd.com/dog/vet-verified/best-dog-food\n",
            "Processing URL 3: https://www.nytimes.com/wirecutter/reviews/how-to-buy-the-best-dog-food/\n",
            "Processing Reddit URL 4: https://www.reddit.com/r/dogs/comments/1hxb7fu/any_good_dog_food_recommendations/\n",
            "Processing URL 5: https://www.consumerreports.org/health/pet-food/whats-really-in-your-dogs-food-a1115304393/\n",
            "CSV file saved: GoogleSearch_dog_food.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Best cat food\"\n",
        "scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename=\"GoogleSearch_cat_food.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3av3d6VVlPC",
        "outputId": "1d9f00d4-566a-4da3-b793-f02b6d4cd0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 10 URLs.\n",
            "Processing URL 1: https://cats.com/cat-food-reviews\n",
            "Processing URL 2: https://www.chewy.com/b/food-387\n",
            "Unable to extract content from https://www.chewy.com/b/food-387, skipping.\n",
            "Processing URL 3: https://www.forbes.com/sites/forbes-personal-shopper/article/best-cat-foods/\n",
            "Processing URL 4: https://www.petmd.com/cat/vet-verified/best-cat-food\n",
            "Processing Reddit URL 5: https://www.reddit.com/r/CatAdvice/comments/1cv5j1r/what_is_the_best_cat_food_price_isnt_a_concern/\n",
            "Processing URL 6: https://www.vet.cornell.edu/departments-centers-and-institutes/cornell-feline-health-center/health-information/feline-health-topics/feeding-your-cat\n",
            "CSV file saved: GoogleSearch_cat_food.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Best cat litter\"\n",
        "scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename=\"GoogleSearch_cat_litter.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4VmrAoaVlPD",
        "outputId": "004133ce-b149-4d9d-f3c0-ea2fd6ea1a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 8 URLs.\n",
            "Processing URL 1: https://www.nytimes.com/wirecutter/reviews/best-cat-litter/\n",
            "Processing Reddit URL 2: https://www.reddit.com/r/CatAdvice/comments/1cmj2l0/whats_the_best_cat_litter/\n",
            "Processing URL 3: https://www.consumerreports.org/home-garden/best-cat-litter-a1408378942/\n",
            "Processing URL 4: https://www.amazon.com/Worlds-Best-Cat-Litter-Scoopable/dp/B007ZPX2NA\n",
            "Unable to extract content from https://www.amazon.com/Worlds-Best-Cat-Litter-Scoopable/dp/B007ZPX2NA, skipping.\n",
            "Processing URL 5: https://www.thesprucepets.com/best-cat-litters-4154326\n",
            "Processing URL 6: https://www.worldsbestcatlitter.com/\n",
            "CSV file saved: GoogleSearch_cat_litter.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging datasets of the five categories into one"
      ],
      "metadata": {
        "id": "V9F1r5WTan28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = glob.glob(\"GoogleSearch_*.csv\")\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Fill empty Result Rank using the original DataFrame's index\n",
        "    df[\"Result Rank\"] = df.groupby(\"Source\")[\"Result Rank\"].transform(\n",
        "        lambda x: x.fillna(pd.Series(range(1, len(x) + 1), index=x.index))\n",
        "    )\n",
        "\n",
        "    # Remove prefix and suffix, then replace underscores with spaces.\n",
        "    category_name = file.replace(\"GoogleSearch_\", \"\").replace(\".csv\", \"\").replace(\"_\", \" \")\n",
        "    df[\"Category\"] = category_name\n",
        "    df_list.append(df)\n",
        "\n",
        "merged_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "merged_df.to_csv(\"GoogleSearch_Data_4_15.csv\", index=False)\n",
        "\n",
        "print(\"All CSV files have been merged into 'GoogleSearch_Data_4_15.csv' with 'Category' column.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdurjLYWTQ6e",
        "outputId": "715b0df8-a5bc-42e8-c43e-e27c6474e023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All CSV files have been merged into 'GoogleSearch_Data_4_15.csv' with 'Category' column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instant Coffee - Google and Bing"
      ],
      "metadata": {
        "id": "R-z5NA7SuB3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Top recommended instant coffee in Canada\"\n",
        "scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename=\"GoogleSearch_instant_coffee.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e6d2a5-6eef-48d7-c4e7-5c833d159f9c",
        "id": "kzKAK9LiuKtf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 18 URLs for engine='google'.\n",
            "Processing Reddit URL 1: https://www.reddit.com/r/BuyCanadian/comments/1ilpd72/best_grocery_store_instant_coffee_brand_made_in/\n",
            "Processing URL 2: https://www.narcity.com/instant-coffee-comparison-canada\n",
            "Processing URL 3: https://www.andreaference.com/blog/the-best-instant-coffee\n",
            "Processing URL 4: https://www.bonappetit.com/story/best-instant-coffee?srsltid=AfmBOooEyIlpse_BETT1nYIYBV8jwfA1ZuCvSxAnTOSgatpHWxXjDzSl\n",
            "Processing URL 5: https://cornercoffeestore.com/best-instant-coffees-canada/\n",
            "Unable to extract content from https://cornercoffeestore.com/best-instant-coffees-canada/, skipping.\n",
            "Processing URL 6: https://www.amazon.ca/best-instant-coffee/s?k=best+instant+coffee\n",
            "Processing URL 7: https://www.allrecipes.com/longform/best-instant-coffee/\n",
            "Processing URL 8: https://justuscoffee.com/collections/favourites/products/organic-instant-coffee-canada\n",
            "Processing URL 9: https://www.thekitchn.com/best-instant-coffee-23632498\n",
            "Processing URL 10: https://www.businessinsider.com/what-is-the-best-instant-coffee-review-2022-2\n",
            "Unable to extract content from https://www.businessinsider.com/what-is-the-best-instant-coffee-review-2022-2, skipping.\n",
            "Processing URL 11: https://www.thespruceeats.com/best-instant-coffee-4690937\n",
            "Processing URL 12: https://www.seriouseats.com/best-instant-coffee-7506045\n",
            "CSV file saved: GoogleSearch_instant_coffee.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_serpapi_products(query, engine='bing',required_count=10, num_results=20, csv_filename=\"BingSearch_instant_coffee.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db2dd3a-363a-4300-c00c-267d344f9565",
        "id": "yjgYdEdouKtg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERPAPI returned 10 URLs for engine='bing'.\n",
            "Processing URL 1: https://cornercoffeestore.com/best-instant-coffees-canada/\n",
            "Unable to extract content from https://cornercoffeestore.com/best-instant-coffees-canada/, skipping.\n",
            "Processing URL 2: https://www.dropmocha.ca/post/top-7-instant-coffee-brands-you-need-to-try-in-canada\n",
            "Processing URL 3: https://www.wired.com/gallery/best-instant-coffee/\n",
            "Processing URL 4: https://www.dropmocha.ca/post/savoring-convenience-the-best-instant-coffees-to-buy-in-canada\n",
            "Processing URL 5: https://www.narcity.com/instant-coffee-comparison-canada\n",
            "Processing URL 6: https://ca.bestreviews.guide/instant-coffees\n",
            "Processing URL 7: https://www.bestproductscanada.com/instant-coffee\n",
            "Processing URL 8: https://grindthosebeans.com/coffee-brand-in-canada/\n",
            "Processing URL 9: https://www.bestratedincanada.com/best-canadian-coffee-brands/\n",
            "Processing URL 10: https://www.homegrounds.co/best-instant-coffee/\n",
            "Warning: fewer than required valid results were scraped.\n",
            "CSV file saved: BingSearch_instant_coffee.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = glob.glob(\"*_instant_coffee.csv\")\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Fill empty Result Rank using the original DataFrame's index\n",
        "    df[\"Result Rank\"] = df.groupby(\"Source\")[\"Result Rank\"].transform(\n",
        "        lambda x: x.fillna(pd.Series(range(1, len(x) + 1), index=x.index))\n",
        "    )\n",
        "\n",
        "    # Remove prefix and suffix, then replace underscores with spaces.\n",
        "    category_name = 'instant coffee'\n",
        "    df[\"Category\"] = category_name\n",
        "    df_list.append(df)\n",
        "\n",
        "SerpAPI_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "SerpAPI_df.to_csv(\"SerpAPI_Data_4_21.csv\", index=False)\n",
        "\n",
        "print(\"All CSV files have been merged into 'SerpAPI_Data_4_21.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_TlM_-guGyC",
        "outputId": "80ba04bc-6020-42b3-9596-f118a091504f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All CSV files have been merged into 'SerpAPI_Data_4_21.csv'.\n"
          ]
        }
      ]
    }
  ]
}