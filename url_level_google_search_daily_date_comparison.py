# -*- coding: utf-8 -*-
"""URL-Level Google Search Daily Date Comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AD6mIjxuqrgeO591OotzxZqzJqsRAKgM

## Google Search daily data comparison

### Comparison based on URLs - 2 days
"""

from urllib.parse import urlparse, urlunparse

def normalize_url(url):
    try:
        parsed = urlparse(url)
        clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path.rstrip("/"), '', '', ''))
        return clean_url.lower()
    except:
        return str(url).lower().strip()

def compare_2_urls_by_category(file1, file2, plot=False):
    df1 = pd.read_csv(file1)
    df2 = pd.read_csv(file2)

    # Add a new column "Normalized URL" by applying the normalize_url function on "Source Link"
    df1["Normalized URL"] = df1["Source Link"].apply(normalize_url)
    df2["Normalized URL"] = df2["Source Link"].apply(normalize_url)

    all_categories = sorted(set(df1["Category"].unique()) | set(df2["Category"].unique()))
    summary = []

    for category in all_categories:
        # Filter rows by category.
        urls1 = df1[df1["Category"] == category]
        urls2 = df2[df2["Category"] == category]

        # Compute sets of normalized URLs.
        urls1_set = set(urls1["Normalized URL"].dropna().unique())
        urls2_set = set(urls2["Normalized URL"].dropna().unique())

        common = urls1_set & urls2_set
        only_in_1 = urls1_set - urls2_set
        only_in_2 = urls2_set - urls1_set

        total_unique = len(urls1_set | urls2_set)
        overlap_rate = len(common) / total_unique if total_unique > 0 else 0

        summary.append({
            "Category": category,
            "URLs in Day1": len(urls1_set),
            "URLs in Day2": len(urls2_set),
            "Common URLs": len(common),
            "Overlap Rate": round(overlap_rate, 2)
        })

        print(f"\nðŸ“¦ Category: {category}")
        print(f"âœ… Common URLs: {len(common)}")

        print(f"\nðŸ”´ Only in {file1} ({len(only_in_1)}):")
        print(
            urls1[urls1["Normalized URL"].isin(only_in_1)]
            [["Source Link", "Source Rank"]]
            .drop_duplicates(subset="Source Link")
            .sort_values("Source Rank")
        )

        print(f"\nðŸ”µ Only in {file2} ({len(only_in_2)}):")
        print(
            urls2[urls2["Normalized URL"].isin(only_in_2)]
            [["Source Link", "Source Rank"]]
            .drop_duplicates(subset="Source Link")
            .sort_values("Source Rank")
        )

        if plot:
            venn2([urls1_set, urls2_set], set_labels=("Day 1", "Day 2"))
            plt.title(f"URL Overlap for {category}")
            plt.show()

    result_df = pd.DataFrame(summary)
    return result_df

googlesearch_3_31 = pd.read_csv("GoogleSearch_Data_3_31.csv")
googlesearch_4_1 = pd.read_csv("GoogleSearch_Data_4_1.csv")
googlesearch_4_2 = pd.read_csv("GoogleSearch_Data_4_2.csv")

result = compare_2_urls_by_category(
    "googlesearch_brand_3_31.csv",
    "googlesearch_brand_4_1.csv",
    plot=True
)

result.sort_values(by="Overlap Rate", ascending=False)

"""### Comparison based on URLs - 3 days"""

def compare_google_urls_by_category_3(file1, file2, file3, plot=False):
    df1 = pd.read_csv(file1)
    df2 = pd.read_csv(file2)
    df3 = pd.read_csv(file3)

    # Add a new column "Normalized URL" by applying normalize_url to "Source Link"
    df1["Normalized URL"] = df1["Source Link"].apply(normalize_url)
    df2["Normalized URL"] = df2["Source Link"].apply(normalize_url)
    df3["Normalized URL"] = df3["Source Link"].apply(normalize_url)

    # Get the union of all categories across the three files.
    all_categories = sorted(set(df1["Category"].unique()) | set(df2["Category"].unique()) | set(df3["Category"].unique()))
    summary = []

    for category in all_categories:
        # Filter rows by category.
        urls1 = df1[df1["Category"] == category]
        urls2 = df2[df2["Category"] == category]
        urls3 = df3[df3["Category"] == category]

        # Compute sets of normalized URLs.
        urls1_set = set(urls1["Normalized URL"].dropna().unique())
        urls2_set = set(urls2["Normalized URL"].dropna().unique())
        urls3_set = set(urls3["Normalized URL"].dropna().unique())

        # Compute common URLs among all three days.
        common_all = urls1_set & urls2_set & urls3_set

        # Compute URLs unique to each day.
        only_in_1 = urls1_set - (urls2_set | urls3_set)
        only_in_2 = urls2_set - (urls1_set | urls3_set)
        only_in_3 = urls3_set - (urls1_set | urls2_set)

        total_unique = len(urls1_set | urls2_set | urls3_set)
        overlap_rate = len(common_all) / total_unique if total_unique > 0 else 0

        summary.append({
            "Category": category,
            "URLs in Day1": len(urls1_set),
            "URLs in Day2": len(urls2_set),
            "URLs in Day3": len(urls3_set),
            "Common URLs": len(common_all),
            "Overlap Rate": round(overlap_rate, 2)
        })

        print(f"\nðŸ“¦ Category: {category}")
        print(f"âœ… Common URLs (all three): {len(common_all)}")

        print(f"\nðŸ”´ Only in {file1} ({len(only_in_1)}):")
        print(urls1[urls1["Normalized URL"].isin(only_in_1)][["Source Link", "Source Rank"]]
              .drop_duplicates(subset="Source Link")
              .sort_values("Source Rank"))

        print(f"\nðŸ”µ Only in {file2} ({len(only_in_2)}):")
        print(urls2[urls2["Normalized URL"].isin(only_in_2)][["Source Link", "Source Rank"]]
              .drop_duplicates(subset="Source Link")
              .sort_values("Source Rank"))

        print(f"\nðŸŸ¢ Only in {file3} ({len(only_in_3)}):")
        print(urls3[urls3["Normalized URL"].isin(only_in_3)][["Source Link", "Source Rank"]]
              .drop_duplicates(subset="Source Link")
              .sort_values("Source Rank"))

        if plot:
            venn3([urls1_set, urls2_set, urls3_set],
                  set_labels=("Day 1", "Day 2", "Day 3"))
            plt.title(f"URL Overlap for {category}")
            plt.show()

    result_df = pd.DataFrame(summary)
    return result_df

result = compare_google_urls_by_category_3("GoogleSearch_Data_3_31.csv", "GoogleSearch_Data_4_1.csv", "GoogleSearch_Data_4_2.csv", plot=True)
print(result)