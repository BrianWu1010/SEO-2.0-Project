# -*- coding: utf-8 -*-
"""URL-Level Comparison between Google Search and Open Perplex API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u0uAba6k7Sf14QtlbUceUKkjL00IcIh8

## Google Search versus Open Perplex

### Preparation
"""

PERPLEXITY_API_KEY = "your perplexity api key"
client_sync = OpenperplexSync(PERPLEXITY_API_KEY)

def extract_product_info(response_text):
    product_info = []
    # Split the response into blocks for each product (assuming each block starts with "Product Title:")
    product_blocks = response_text.strip().split('\n\n')
    for block in product_blocks:
        # Look for each product's information using the exact format
        title_match = re.search(r"Product Title:\s*(.*)", block)
        description_match = re.search(r"Description:\s*(.*)", block)
        brand_match = re.search(r"Brand:\s*(.*)", block)
        retailer_match = re.search(r"Retailer:\s*(.*)", block)
        source_match = re.search(r"Source URL:\s*(http[s]?://\S+)", block)
        comments_match = re.search(r"Comments:\s*(.*)", block)
        # Extract the details if they are found
        title = title_match.group(1) if title_match else "N/A"
        description = description_match.group(1) if description_match else "N/A"
        brand = brand_match.group(1) if brand_match else "N/A"
        retailer = retailer_match.group(1) if retailer_match else "N/A"
        source = source_match.group(1) if source_match else "N/A"
        comments = comments_match.group(1) if comments_match else "N/A"
        product_info.append({
            "Title": title,
            "Description": description,
            "Brand": brand,
            "Retailer": retailer,
            "Source": source,
            "Comments": comments
        })
    return product_info

def fetch_perplexity_products(query, csv_filename, product_num=10):
    """
    For a given query, fetch the top product_num best products using the Perplexity API,
    extract product information from the structured response, and save the results to a CSV file.

    Parameters:
        query (str): The base query string.
        csv_filename (str): The output CSV file name.
        product_num (int): The number of products to fetch. Default is 20.

    Returns:
        pd.DataFrame: DataFrame containing the extracted product info.
    """
    data = []
    # Build the full query prompt by inserting the query.
    full_query = f"""
    Provide the top {product_num} {query}.
    For each product, return the following details in this exact format:

    Product Title: [Title]
    Description: [Description]
    Brand: [Brand]
    Retailer: [Retailer]
    Source URL: [URL]
    Comments: [Comments]

    Make sure to return the details for all products in the exact format above, one after the other.
    IMPORTANT: Please include data for all these keys.
    """

    print(f"Perplexity Web Searching: {full_query}")

    try:
        response = client_sync.search(
            query=full_query,
            model='o3-mini-high',
            response_language="en",
            answer_type="text"
        )
        print("API Response:", response)
        # Get the LLM response text.
        response_text = response.get("llm_response", "")
        # Extract product information using your helper function.
        product_info = extract_product_info(response_text)
        # If product info was found, append it to data with a common timestamp.
        if product_info:
            timestamp = datetime.now(local_tz).strftime("%Y-%m-%d %H:%M:%S")
            for info in product_info:
                data.append({
                    "Search Source": "Perplexity",
                    "Title": info["Title"],
                    "Description": info["Description"],
                    "Brand": info["Brand"],
                    "Retailer": info["Retailer"],
                    "Source": info["Source"],
                    "Comments": info["Comments"],
                    "Query": query,
                    "Timestamp": timestamp
                })
            time.sleep(2)
    except Exception as e:
        print(f"Error during Perplexity API call for {query}: {e}")

    if data:
        df = pd.DataFrame(data)
        df.to_csv(csv_filename, index=False)
        print(f"CSV file saved successfully as '{csv_filename}'!")
    else:
        print("No data was collected. Please check your API key, network connection, and query parameters.")
        return pd.DataFrame()

from urllib.parse import urlparse, urlunparse

def normalize_url(url):
    try:
        parsed = urlparse(url)
        clean_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path.rstrip("/"), '', '', ''))
        return clean_url.lower()
    except:
        return str(url).lower().strip()

def compare_2_urls_by_category(googlefile, llmfile, plot=False):
    df1 = pd.read_csv(googlefile)
    df2 = pd.read_csv(llmfile)

    # Add a new column "Normalized URL" by applying the normalize_url function on "Source"
    df1["Normalized URL"] = df1["Source"].apply(normalize_url)
    df2["Normalized URL"] = df2["Source"].apply(normalize_url)

    all_categories = sorted(set(df1["Category"].unique()) | set(df2["Category"].unique()))
    summary = []

    for category in all_categories:
        # Filter rows by category.
        urls1 = df1[df1["Category"] == category]
        urls2 = df2[df2["Category"] == category]

        # Compute sets of normalized URLs.
        urls1_set = set(urls1["Normalized URL"].dropna().unique())
        urls2_set = set(urls2["Normalized URL"].dropna().unique())

        common = urls1_set & urls2_set
        only_in_1 = urls1_set - urls2_set
        only_in_2 = urls2_set - urls1_set

        total_unique = len(urls1_set | urls2_set)
        overlap_rate = len(common) / total_unique if total_unique > 0 else 0

        summary.append({
            "Category": category,
            "URLs in Google Search Result": len(urls1_set),
            "URLs in LLM result": len(urls2_set),
            "Common URLs": len(common),
            "Highest Source Rank of Common URLs in Google Search Results": urls1[urls1['Normalized URL'].isin(common)]['Source Rank'].min() if common else "N/A",
            "Overlap Rate": f"{round(overlap_rate, 2)*100}%"
        })

        print(f"\nðŸ“¦ Category: {category}")
        print(f"âœ… Common URLs: {len(common)}")
        if len(common) > 0:
            print(f"Highest Source Rank of the Common URLs in Google Search results: {urls1[urls1['Normalized URL'].isin(common)]['Source Rank'].min()}")

        print(f"\nðŸ”´ Only in {googlefile} ({len(only_in_1)}):")
        print(
            urls1[urls1["Normalized URL"].isin(only_in_1)]
            [["Source", "Source Rank"]]
            .drop_duplicates(subset="Source")
            .sort_values("Source Rank")
        )

        print(f"\nðŸ”µ Only in {llmfile} ({len(only_in_2)}):")
        print(
            urls2[urls2["Normalized URL"].isin(only_in_2)]
            # [["Source", "Source Rank"]]
            [["Source"]]
            .drop_duplicates(subset="Source")
            # .sort_values("Source Rank")
        )

        if plot:
            venn2([urls1_set, urls2_set], set_labels=("Google Search URLs", "Open Perplexity URLs"))
            plt.title(f"URL Overlap for {category}")
            plt.show()

    result_df = pd.DataFrame(summary)
    return result_df

"""### Five Categories"""

local_tz = pytz.timezone("America/Toronto")

def save_serpapi_urls_to_csv(query, num_results=10, output_filename="serpapi_sources.csv"):
    timestamp = datetime.now(local_tz).strftime("%Y-%m-%d %H:%M:%S")
    urls = get_serpapi_urls(query, num_results=num_results)

    with open(output_filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Query", "Source", "Source Rank", "Timestamp", "Search Source"])

        for rank, url in enumerate(urls, start=1):
            writer.writerow([query, url, rank, timestamp, "Google Search"])

    print(f"âœ… CSV file saved as: {output_filename}")

query = "best instant coffee"
get_serpapi_urls(query, num_results=10)
# save_serpapi_urls_to_csv(query, num_results=10, output_filename="GoogleSearch_instant_coffee.csv")
# fetch_perplexity_products(query, csv_filename = "perplex_instant_coffee.csv", product_num=10)

query = "best baking ingredients"
save_serpapi_urls_to_csv(query, num_results=10, output_filename="GoogleSearch_baking_ingredients.csv")
# fetch_perplexity_products(query, csv_filename = "perplex_baking_ingredients.csv", product_num=10)

query = "best cat food"
save_serpapi_urls_to_csv(query, num_results=10, output_filename="GoogleSearch_cat_food.csv")
# fetch_perplexity_products(query, csv_filename = "perplex_cat_food.csv", product_num=10)

query = "best dog food"
save_serpapi_urls_to_csv(query, num_results=10, output_filename="GoogleSearch_dog_food.csv")
# fetch_perplexity_products(query, csv_filename = "perplex_dog_food.csv", product_num=10)

query = "best cat litter"
save_serpapi_urls_to_csv(query, num_results=10, output_filename="GoogleSearch_cat_litter.csv")
# fetch_perplexity_products(query, csv_filename = "perplex_cat_litter.csv", product_num=10)

"""### Merge five categories"""

csv_files = glob.glob("GoogleSearch_*.csv")

df_list = []

for file in csv_files:
    df = pd.read_csv(file)

    # Remove prefix and suffix, then replace underscores with spaces.
    category_name = file.replace("GoogleSearch_", "").replace(".csv", "").replace("_", " ")
    df["Category"] = category_name
    df_list.append(df)

merged_perplexity_df = pd.concat(df_list, ignore_index=True)

merged_perplexity_df.to_csv("merged_GoogleSearch_Data_4_15_1800.csv", index=False)

print("All CSV files have been merged into 'merged_GoogleSearch_Data_4_15_1800.csv'.")

# csv_files = glob.glob("perplex_*.csv")

# df_list = []

# for file in csv_files:
#     df = pd.read_csv(file)

#     # Remove prefix and suffix, then replace underscores with spaces.
#     category_name = file.replace("perplex_", "").replace(".csv", "").replace("_", " ")
#     df["Category"] = category_name
#     df_list.append(df)

# merged_perplexity_df = pd.concat(df_list, ignore_index=True)

# merged_perplexity_df.to_csv("merged_perplex_Data_4_8.csv", index=False)

# print("All CSV files have been merged into 'merged_perplex_Data_4_8.csv'.")

"""### Comparison"""

compare_2_urls_by_category('merged_GoogleSearch_Data_4_10.csv', 'merged_GoogleSearch_Data_4_15_1652.csv', plot=False)