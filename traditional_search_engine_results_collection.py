# -*- coding: utf-8 -*-
"""Traditional Search Engine Results Collection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IwcC8O9Gr14odFJ7BVNXwwWoj0so9QHG

## Packages prep
"""

pip install openai==0.28

pip install google-search-results pandas

pip install requests

!pip install praw

!pip install --upgrade openperplex

import requests
from bs4 import BeautifulSoup
import re
import time
import json
import csv
import openai
import praw
from serpapi import GoogleSearch
from datetime import datetime
import pytz
local_tz = pytz.timezone("America/Toronto")
import glob
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib_venn import venn2, venn3
from openperplex import OpenperplexSync

# Set your API keys
openai.api_key = "your openai api key"
SERPAPI_API_KEY = "your serpapi key"

"""# Google Searching Data Prep

## Helper functions
"""

# Define headers for requests
headers = {"User-Agent": "Mozilla/5.0"}

def get_serpapi_urls(query, engine='google', num_results=10):
    """Return a list of website URLs from SERPAPI using the given query."""
    params = {
        "engine": engine,
        "q": query,
        "cc": "CA",
        "api_key": SERPAPI_API_KEY,
        "num": num_results
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    organic_results = results.get("organic_results", [])
    urls = []
    for res in organic_results:
        url = res.get("link", None)
        if url:
            urls.append(url)
    return urls

def get_cleaned_html(url):
    """Attempt to scrape and clean the HTML from a given URL."""
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        # Remove script and style tags
        for tag in soup(["style"]):
            tag.decompose()
        # Try to extract main content: first look for <article>
        article = soup.find("article")
        if not article:
            # Fallback: try a div with class "content"
            article = soup.find("div", {"class": "content"})
        if not article:
            # As last resort, use the whole page
            article = soup
        cleaned = article.get_text(separator="\n", strip=True)
        return cleaned
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

def extract_product_data(cleaned_html):
    """
    Use GPT-4o to extract product ranking data from the cleaned HTML.
    This function calls the model with function_call parameters and returns a dictionary.
    """
    try:
        completion = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an expert at extracting product rankings and critical reviews from article HTML content. "
                        # "Focus on extracting subjective evaluations, opinions, and direct comparisons between products rather than just product descriptions. "
                        # "Critical reviews should include strengths, weaknesses, pros, and cons mentioned about each product. "
                        # "Ignore neutral descriptions that only state product features without an evaluation. "
                        "If explicit rankings exist, maintain them. If no ranking is mentioned, rank the products in the order they appear in the article. Never leave it blank."
                        "If there are more than 10 products mentioned, only keep the top 10."
                    )

                },
                {"role": "user", "content": cleaned_html}
            ],
            functions=[
                {
                    "name": "parse_product_data",
                    "description": (
                        "Extracts ranked products and their corresponding critical reviews from article HTML content. "
                        "If explicit rankings exist, keep them. If no ranking is given, rank them by the order of appearance. Never leave it blank."
                        "If a product has no review in the article, mark it as 'No review available' instead of leaving it empty."
                    ),
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "products": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "Result Rank": {"type": "integer"},
                                        "Product Title": {"type": "string"},
                                        "Comment": {"type": "string"}
                                    }
                                }
                            }
                        }
                    }
                }
            ],
            function_call={"name": "parse_product_data"}
        )
        func_call = completion.choices[0].message.get("function_call", {})
        arguments_str = func_call.get("arguments", "{}")
        data = json.loads(arguments_str)
        return data.get("products", [])
    except Exception as e:
        print(f"Error extracting product data: {e}")
        return None

def get_reddit_comments(url):
    """
    Fetch the top 10 comments from a given Reddit URL.
    """
    reddit = praw.Reddit(
        client_id="ZUwpxO018ZJGE4E-dfwazQ",
        client_secret="SUE9d8Q_wr_4N1qcebomwtS293GwJw",
        user_agent="MyRedditScraper/0.1",
        check_for_async=False
    )

    submission = reddit.submission(url=url)
    submission.comments.replace_more(limit=0)
    comments = submission.comments.list()
    # Sort comments by score descending, take top 10.
    top_comments = sorted(comments, key=lambda x: x.score, reverse=True)[:10]
    # Concatenate comment texts with their score.
    comments_text = "\n\n".join([f"Score: {c.score}\nComment: {c.body}" for c in top_comments])
    return comments_text

def extract_product_data_from_reddit(url):
    """
    Use GPT-4o to extract product ranking data from the Reddit comments.
    This function calls the model with function_call parameters and returns a list of product dictionaries.
    Only products with positive (or at least non-negative) reviews are returned;
    if a product is judged to have a negative review, it is skipped.
    """
    try:
        reddit_comments = get_reddit_comments(url)
        # Call OpenAI to analyze the Reddit comments.
        # Note: The system prompt instructs the model to only return products that are positively reviewed.
        completion = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an expert at extracting product names, rankings and critical reviews information from aggregated Reddit comments. "
                        "Analyze the following top 10 comments and extract the product ranking, product name, a summary of the critical reviews (Comment). "
                        "Only include products that are positively reviewed. If a product is judged to have a negative review, do not include it in the results. "
                        "Rank products in the order they appear."
                        "If a product has no clear review, mark it as 'No review available'."
                    )
                },
                {"role": "user", "content": reddit_comments}
            ],
            functions=[
                {
                    "name": "parse_product_data",
                    "description": (
                        "Extracts ranked products, and their corresponding critical reviews from Reddit comments. "
                        "Only return products with positive reviews. Otherwise, maintain the order of appearance."
                    ),
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "products": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "Result Rank": {"type": "integer"},
                                        "Product Title": {"type": "string"},
                                        "Comment": {"type": "string"}
                                    }
                                }
                            }
                        }
                    }
                }
            ],
            function_call={"name": "parse_product_data"}
        )
        func_call = completion.choices[0].message.get("function_call", {})
        arguments_str = func_call.get("arguments", "{}")
        data = json.loads(arguments_str)
        return data.get("products", [])
    except Exception as e:
        print(f"Error extracting product data from reddit URL {url}: {e}")
        return None

def scrape_serpapi_products(query, engine='google', required_count=5, num_results=10, csv_filename="output.csv"):
    """
    Uses SERPAPI to fetch website URLs based on the given query and engine, scrapes them to extract product data,
    and writes the results to a CSV file. Each product record is augmented with:
        - 'Source': the URL from which the data was scraped.
        - 'Source Rank': the ordinal number (1-based) of the SERPAPI URL that produced this data.
        - 'Search Source': the engine name (e.g. "google", "google_shopping", etc.)
        - 'Query': the SERPAPI query string used.
        - 'Timestamp': the timestamp when the query was sent to SERPAPI.

    The final CSV file will contain the following columns:
      - Result Rank
      - Title        (was "Product Title")
      - Description  (was "Comment")
      - Source       (was "Source Link")
      - Source Rank
      - Search Source
      - Query
      - Timestamp

    Parameters:
        query (str): The SERPAPI query string.
        engine (str): The SERPAPI engine to use (e.g. "google", "google_shopping").
        required_count (int): The number of successfully scraped website results to collect.
        num_results (int): The number of URLs to ask SERPAPI for.
        csv_filename (str): The filename for the CSV output.
    """
    from datetime import datetime
    import time, csv

    query_used = query
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Fetch URLs using the specified engine
    urls = get_serpapi_urls(query, engine, num_results=num_results)
    print(f"SERPAPI returned {len(urls)} URLs for engine='{engine}'.")

    results_set = set()

    for i, url in enumerate(urls, start=1):
        # handle Reddit specially
        if "reddit" in url.lower():
            print(f"Processing Reddit URL {i}: {url}")
            products_data = extract_product_data_from_reddit(url)
        else:
            if len(results_set) >= required_count:
                break
            print(f"Processing URL {i}: {url}")
            cleaned_html = get_cleaned_html(url)
            if not cleaned_html or len(cleaned_html) < 100:
                print(f"Unable to extract content from {url}, skipping.")
                continue
            products_data = extract_product_data(cleaned_html)

        if not products_data:
            print(f"Extraction failed for {url}, skipping.")
            continue

        for product in products_data:
            product["Source Link"] = url
            product["Source Rank"] = i
            product["Timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # ensure uniqueness
        hashable = tuple(frozenset(prod.items()) for prod in products_data)
        results_set.add(hashable)
        time.sleep(1)

    if len(results_set) < required_count:
        print("Warning: fewer than required valid results were scraped.")

    # flatten
    all_products = []
    for group in results_set:
        for item in group:
            all_products.append(dict(item))

    # write CSV
    with open(csv_filename, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([
            "Result Rank",
            "Title",
            "Description",
            "Source",
            "Source Rank",
            "Search Source",
            "Query",
            "Timestamp"
        ])
        for prod in all_products:
            writer.writerow([
                prod.get("Result Rank", "N/A"),
                prod.get("Product Title", "N/A"),   # renamed to Title
                prod.get("Comment", "N/A"),         # renamed to Description
                prod.get("Source Link", "N/A"),     # renamed to Source
                prod.get("Source Rank", "N/A"),
                engine,                             # Search Source comes from the engine parameter
                query_used,
                prod.get("Timestamp", timestamp)
            ])

    print(f"CSV file saved: {csv_filename}")

"""## Five categories - 10 URLs"""

# query = "Best baking ingredients"
# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename="GoogleSearch_baking_ingredients.csv")

query = "Top recommended instant coffee in Canada"
scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename="GoogleSearch_instant_coffee.csv")

# query = "Best dog food"
# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename="GoogleSearch_dog_food.csv")

# query = "Best cat food"
# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename="GoogleSearch_cat_food.csv")

# query = "Best cat litter"
# scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename="GoogleSearch_cat_litter.csv")

"""## Five categories - 5 URLs"""

query = "Best baking ingredients"
scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename="GoogleSearch_baking_ingredients.csv")

query = "Best instant coffee"
scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename="GoogleSearch_instant_coffee.csv")

query = "Best dog food"
scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename="GoogleSearch_dog_food.csv")

query = "Best cat food"
scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename="GoogleSearch_cat_food.csv")

query = "Best cat litter"
scrape_serpapi_products(query, required_count=5, num_results=10, csv_filename="GoogleSearch_cat_litter.csv")

"""## Merging datasets of the five categories into one"""

csv_files = glob.glob("GoogleSearch_*.csv")

df_list = []

for file in csv_files:
    df = pd.read_csv(file)

    # Fill empty Result Rank using the original DataFrame's index
    df["Result Rank"] = df.groupby("Source")["Result Rank"].transform(
        lambda x: x.fillna(pd.Series(range(1, len(x) + 1), index=x.index))
    )

    # Remove prefix and suffix, then replace underscores with spaces.
    category_name = file.replace("GoogleSearch_", "").replace(".csv", "").replace("_", " ")
    df["Category"] = category_name
    df_list.append(df)

merged_df = pd.concat(df_list, ignore_index=True)

merged_df.to_csv("GoogleSearch_Data_4_15.csv", index=False)

print("All CSV files have been merged into 'GoogleSearch_Data_4_15.csv' with 'Category' column.")

"""## Instant Coffee - Google and Bing"""

query = "Top recommended instant coffee in Canada"
scrape_serpapi_products(query, required_count=10, num_results=20, csv_filename="GoogleSearch_instant_coffee.csv")

scrape_serpapi_products(query, engine='bing',required_count=10, num_results=20, csv_filename="BingSearch_instant_coffee.csv")

csv_files = glob.glob("*_instant_coffee.csv")

df_list = []

for file in csv_files:
    df = pd.read_csv(file)

    # Fill empty Result Rank using the original DataFrame's index
    df["Result Rank"] = df.groupby("Source")["Result Rank"].transform(
        lambda x: x.fillna(pd.Series(range(1, len(x) + 1), index=x.index))
    )

    # Remove prefix and suffix, then replace underscores with spaces.
    category_name = 'instant coffee'
    df["Category"] = category_name
    df_list.append(df)

SerpAPI_df = pd.concat(df_list, ignore_index=True)

SerpAPI_df.to_csv("SerpAPI_Data_4_21.csv", index=False)

print("All CSV files have been merged into 'SerpAPI_Data_4_21.csv'.")